{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0d78702b888709a3d1d4028e41e80e36ee8fbe81cbfebb84acac8fa5249620c56",
   "display_name": "Python 3.8.5 64-bit ('core.wandb': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-05-27 15:29:53,587\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33makamlani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "wandb version: 0.10.30\n",
      "env: \"WANDB_NOTEBOOK_NAME\"=\"m5-ts-model\"\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy  as np\n",
    "import glob \n",
    "import os \n",
    "import gc \n",
    "from   pathlib import Path\n",
    "from   typing import  List, Tuple, Callable, Any \n",
    "from   dataclasses import dataclass, asdict, field\n",
    "from   pprint import pprint\n",
    "from   functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "\n",
    "import modin.pandas as pd \n",
    "import ray \n",
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "import wandb\n",
    "print(f'wandb version: {wandb.__version__}')\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"m5-ts-model\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from    utils.config import load_config\n",
    "from    utils.models import ModelForecastDense, ModelForecastBiLSTM, ModelForecastLSTM, ModelForecastLSTMOneShot\n",
    "\n",
    "import  flexassist.integrations.wandb.lifecycle       as lc \n",
    "import  flexassist.integrations.sklearn.encoder       as enc\n",
    "import  flexassist.integrations.tensorflow.dataloader as dl \n",
    "import  flexassist.integrations.tensorflow.encoder    as tfenc\n",
    "import  flexassist.integrations.tensorflow.modeler    as tfmdl\n",
    "import  flexassist.integrations.tensorflow.trainer    as trainer\n",
    "import  flexassist.core.system.writer                 as wr "
   ]
  },
  {
   "source": [
    "## Ingestion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "config_files:List[str] = glob.glob('./config' + '/*')\n",
    "config = load_config('./config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(30490, 1947)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "datasets_dir   = Path(config['experiment_parameters']['datasets_dir'])\n",
    "transform_dir  = Path(datasets_dir)/'transformers'\n",
    "df_cal         = pd.read_csv(Path(transform_dir)/'calendar.csv')    \n",
    "df_prices      = pd.read_csv(Path(transform_dir)/'sell_prices.csv')    \n",
    "df_train       = pd.read_csv(Path(transform_dir)/'train.csv') \n",
    "df_train.shape   "
   ]
  },
  {
   "source": [
    "## Transforms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into just time series\n",
    "ts_cols               = [col for col in df_train.columns if 'd_' in col]\n",
    "lvl_cols              = [col for col in df_train.columns if 'd_' not in col]\n",
    "df_train_daily_qty    = df_train[ts_cols].T\n",
    "# configure time series\n",
    "start_day             = config['architecture']['start_period']    # to avoid many days that have \n",
    "lookback_period       = config['architecture']['lookback_period'] # len(df_train_daily_qty) - start_day; to avoid those days with no transactions\n",
    "time_steps            = config['architecture']['time_steps']      # how many lags to account for \n",
    "horizon               = config['architecture']['horizon']         # how many timesteps to project into future\n",
    "output_width          = config['architecture']['output_width']    # for one week worth of ouput predictions\n",
    "# calculate remaining days\n",
    "df_train_daily_qty_lb = df_train_daily_qty[-lookback_period:]\n",
    "num_days              = len(df_train_daily_qty_lb.index)\n",
    "# calculate partitions\n",
    "val_part_size         = horizon * 6\n",
    "test_part_size        = horizon\n",
    "df_test_part          = df_train_daily_qty_lb[-test_part_size:]                 # Days(1914 - 1941) ~ 1 month\n",
    "df_valid_part         = df_train_daily_qty_lb[-val_part_size:-test_part_size]   # Days(1774 - 1913) ~ 6 months\n",
    "df_train_part         = df_train_daily_qty_lb[0:-val_part_size]                 # Days(351  - 1773) ~ 2.3 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Partition Sizes: (1423, 30490), (140, 30490), (28, 30490)\nTraining Data: min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "df_train_part_sc = scaler.fit_transform(df_train_part)\n",
    "df_valid_part_sc = scaler.transform(df_valid_part)\n",
    "df_test_part_sc  = scaler.transform(df_test_part)\n",
    "print(f\"Partition Sizes: {df_train_part_sc.shape}, {df_valid_part_sc.shape}, {df_test_part_sc.shape}\")\n",
    "# get properties of normalize the the continuous columns\n",
    "# scaler_props = zip([np.min, np.max], [scaler.data_min_, scaler.data_max_])\n",
    "# scaler_props = zip([np.mean, np.var, np.std], [scaler.mean_, scaler.var_, np.sqrt(scaler.var_)])\n",
    "# scaler_props = {k.__name__: v[-1].round(4) for k,v in scaler_props}\n",
    "# print(f\"Scaler Properties: {scaler_props}\")\n",
    "#print(f\"Training Data: mu:  {df_train_part_sc.mean().round(5)}, std: {df_train_part_sc.std().round(5)}\")\n",
    "print(f\"Training Data: min: {df_train_part_sc.min().round(5)}, max: {df_train_part_sc.max().round(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n",
      "UserWarning: `to_csv` defaulting to pandas implementation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'experiments/snapshots/m5-fcst-base/exports/artifacts/datasets/dataprep/test_normalized.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "dataprep_dir = Path(datasets_dir)/'dataprep'\n",
    "wr.write_csv(pd.DataFrame(df_train_part_sc), dataprep_dir, 'training_normalized.csv')    \n",
    "wr.write_csv(pd.DataFrame(df_valid_part_sc), dataprep_dir, 'validation_normalized.csv')    \n",
    "wr.write_csv(pd.DataFrame(df_test_part_sc),  dataprep_dir, 'test_normalized.csv')    "
   ]
  },
  {
   "source": [
    "## Time Series Models "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class SearchQuery:\n",
    "    model_cls:Any\n",
    "    gen_name:str \n",
    "    gen_params:dict \n",
    "    model_name:str               = field(default='model_template') \n",
    "    tracker_task:lc.JobTrackInfo = None \n",
    "\n",
    "def generator(df:pd.DataFrame, features:List[int], num_timesteps:int, out_width:int, horizon:int) -> dict:\n",
    "    \"greate a generator based on input configuration\"\n",
    "    meta_config:dict = dict (\n",
    "        num_timesteps = num_timesteps,\n",
    "        features      = features, \n",
    "        num_features  = len(features),\n",
    "        input_shape   = (num_timesteps, len(features)),\n",
    "        label_width   = out_width,\n",
    "        horizon       = horizon \n",
    "    )\n",
    "    gen = dl.WindowDataLoader(input_width=num_timesteps, label_width=out_width, horizon=horizon)\n",
    "    # data frame is based on training configuration\n",
    "    gen.register_frame(df, features)\n",
    "    gen.register_metadata(meta_config)\n",
    "    return gen\n",
    "\n",
    "def train_and_evaluate(gen, partition_data:tuple, model, model_name:str, configuration:dict, verbose:bool=False):\n",
    "    \"\"\"train and evaluate model on dataset generators\n",
    "\n",
    "    Examples:\n",
    "    >>> features = [19,24,48]\n",
    "    >>> single_step_win = generator(df_train_part_sc, features, num_timesteps=1, out_width=1, horizon=1)\n",
    "    >>> partitions = (df_train_part_sc, df_valid_part_sc, df_test_part_sc)\n",
    "    >>> model, df  = train_and_evaluate(single_step_win, partitions, model=model_dense, model_name='dense', configuration=config)\n",
    "    \"\"\"\n",
    "    # generators \n",
    "    batch_size = configuration['training_parameters']['batch_size']\n",
    "    train, valid, test = partition_data\n",
    "    train_gen = gen.make_dataset(train[:, gen.column_indices], shuffle_en=True,  batch_size=batch_size)\n",
    "    valid_gen = gen.make_dataset(valid[:, gen.column_indices], shuffle_en=False, batch_size=batch_size)\n",
    "    test_gen  = gen.make_dataset(test[:,  gen.column_indices], shuffle_en=False, batch_size=batch_size)\n",
    "    if verbose:\n",
    "        get_dataset_spec  = lambda dataset: dataset.element_spec\n",
    "        print(get_dataset_spec(train_gen))\n",
    "    # training \n",
    "    trainer_svc = trainer.Trainer(configuration)\n",
    "    model, df_history = trainer_svc.train(model_name, model, train_gen, valid_gen, **{'cb_wandb':True})\n",
    "    # evaluation (during training, we use validation data)\n",
    "    scores    = model.evaluate(valid_gen, return_dict=True)\n",
    "    df_scores = pd.DataFrame.from_dict(scores, orient='index', columns=['score']).round(3).T\n",
    "    return model, df_history, df_scores \n",
    "\n",
    "def search(model_data:List[SearchQuery], df:pd.DataFrame, partition_data:tuple, feature_cols:List[int], configuration:dict):\n",
    "    \"iterates and searches over model data for training and scoring\"\n",
    "    df_scoring = pd.DataFrame()\n",
    "    for query in model_data:\n",
    "        gen_win = generator(pd.DataFrame(df), features=feature_cols, **query.gen_params)\n",
    "        cfg     = gen_win.meta_config\n",
    "        model   = query.model_cls (\n",
    "            input_shape     = cfg['input_shape'], \n",
    "            num_timesteps   = cfg['num_timesteps'],\n",
    "            output_width    = cfg['label_width'],\n",
    "            num_features    = cfg['num_features'],\n",
    "            horizon         = cfg['horizon']\n",
    "        )\n",
    "        # lifefycle runner, search over models \n",
    "        with lc.LifeCycleSvc.execute(query.tracker_task) as cxt: \n",
    "            model, df_history, df_scores = train_and_evaluate (\n",
    "                gen             = gen_win, \n",
    "                partition_data  = partition_data, \n",
    "                model           = model, \n",
    "                model_name      = query.model_name, \n",
    "                configuration   = configuration\n",
    "            )\n",
    "            # scoring on the validation set\n",
    "            df_scoring = df_scoring.append(df_scores)\n",
    "            df_scoring = df_scoring.rename(index={'score':query.model_name})\n",
    "    df_scoring = df_scoring.rename_axis('scores')\n",
    "    return df_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Lifecycle Context Runner Defaults \n",
    "# Define init configuration lifecycle context for each job \n",
    "project_config  = config['project']\n",
    "tracker = lc.JobTrackInfo(**dict(\n",
    "    name    = '{}:{}',\n",
    "    project = project_config['name'], \n",
    "    entity  = project_config['entity'],\n",
    "    group   = \":\".join(['exp', 'modeltype', 'timeseries']),\n",
    "    # default main configuration used which will be added to \n",
    "    config  = config, \n",
    "    tags    = project_config['tags'] + ['model experimentation'],\n",
    "    notes   = \"service model experimentation execution\"\n",
    "))\n",
    "tracker.format_job_type(cycle=lc.JobTrackingCycle.Experiment, action=lc.ActionType.Execution)\n",
    "# selecting top 25 of items to train models on\n",
    "features    = pd.read_csv(Path(transform_dir)/'top_id_indices.csv')['indice'].values.tolist()   \n",
    "partitions  = (df_train_part_sc, df_valid_part_sc, df_test_part_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'NoneType'> object. This may take some time.\nUserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastdense:single_in_step:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/jqmw4r1z\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/jqmw4r1z</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153620-jqmw4r1z</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 1/45 [..............................] - ETA: 33s - loss: 0.1080 - mean_absolute_error: 0.2830 - mean_squared_error: 0.1080 - root_mean_squared_error: 0.3287WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0016s vs `on_train_batch_end` time: 0.0058s). Check your callbacks.\n",
      "40/45 [=========================>....] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.1316 - mean_squared_error: 0.0315 - root_mean_squared_error: 0.1774\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.0298 - mean_absolute_error: 0.1276 - mean_squared_error: 0.0298 - root_mean_squared_error: 0.1727 - val_loss: 0.0111 - val_mean_absolute_error: 0.0789 - val_mean_squared_error: 0.0111 - val_root_mean_squared_error: 0.1055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0136 - mean_absolute_error: 0.0872 - mean_squared_error: 0.0136 - root_mean_squared_error: 0.1164 - val_loss: 0.0104 - val_mean_absolute_error: 0.0755 - val_mean_squared_error: 0.0104 - val_root_mean_squared_error: 0.1017\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0122 - mean_absolute_error: 0.0823 - mean_squared_error: 0.0122 - root_mean_squared_error: 0.1105 - val_loss: 0.0095 - val_mean_absolute_error: 0.0719 - val_mean_squared_error: 0.0095 - val_root_mean_squared_error: 0.0974\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0113 - mean_absolute_error: 0.0789 - mean_squared_error: 0.0113 - root_mean_squared_error: 0.1064 - val_loss: 0.0090 - val_mean_absolute_error: 0.0700 - val_mean_squared_error: 0.0090 - val_root_mean_squared_error: 0.0950\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0109 - mean_absolute_error: 0.0771 - mean_squared_error: 0.0109 - root_mean_squared_error: 0.1044 - val_loss: 0.0090 - val_mean_absolute_error: 0.0699 - val_mean_squared_error: 0.0090 - val_root_mean_squared_error: 0.0947\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0106 - mean_absolute_error: 0.0761 - mean_squared_error: 0.0106 - root_mean_squared_error: 0.1031 - val_loss: 0.0089 - val_mean_absolute_error: 0.0692 - val_mean_squared_error: 0.0089 - val_root_mean_squared_error: 0.0941\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_absolute_error: 0.0753 - mean_squared_error: 0.0104 - root_mean_squared_error: 0.1022 - val_loss: 0.0088 - val_mean_absolute_error: 0.0689 - val_mean_squared_error: 0.0088 - val_root_mean_squared_error: 0.0939\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0103 - mean_absolute_error: 0.0748 - mean_squared_error: 0.0103 - root_mean_squared_error: 0.1015 - val_loss: 0.0088 - val_mean_absolute_error: 0.0687 - val_mean_squared_error: 0.0088 - val_root_mean_squared_error: 0.0936\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastdense:single_in_step/assets\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0088 - mean_absolute_error: 0.0687 - mean_squared_error: 0.0088 - root_mean_squared_error: 0.0936\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: `DataFrame.append` for empty DataFrame defaulting to pandas implementation.\n",
      "UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48120<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153620-jqmw4r1z/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153620-jqmw4r1z/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-153624/train/global_step</td><td>7</td></tr><tr><td>_timestamp</td><td>1622144232.68938</td></tr><tr><td>global_step</td><td>360</td></tr><tr><td>_step</td><td>32</td></tr><tr><td>_runtime</td><td>58</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>loss</td><td>0.0103</td></tr><tr><td>mean_absolute_error</td><td>0.07478</td></tr><tr><td>mean_squared_error</td><td>0.0103</td></tr><tr><td>root_mean_squared_error</td><td>0.10148</td></tr><tr><td>val_loss</td><td>0.00875</td></tr><tr><td>val_mean_absolute_error</td><td>0.06869</td></tr><tr><td>val_mean_squared_error</td><td>0.00875</td></tr><tr><td>val_root_mean_squared_error</td><td>0.09356</td></tr><tr><td>best_val_loss</td><td>0.00875</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>20210527-153624/validation/global_step</td><td>7</td></tr><tr><td>20210527-153624/validation/evaluation_loss_vs_iterations</td><td>0.00875</td></tr><tr><td>20210527-153624/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.06869</td></tr><tr><td>20210527-153624/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.00875</td></tr><tr><td>20210527-153624/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.09356</td></tr><tr><td>20210527-153624/train/epoch_loss</td><td>0.0103</td></tr><tr><td>20210527-153624/train/epoch_mean_absolute_error</td><td>0.07478</td></tr><tr><td>20210527-153624/train/epoch_mean_squared_error</td><td>0.0103</td></tr><tr><td>20210527-153624/train/epoch_root_mean_squared_error</td><td>0.10148</td></tr><tr><td>20210527-153624/validation/epoch_loss</td><td>0.00875</td></tr><tr><td>20210527-153624/validation/epoch_mean_absolute_error</td><td>0.06869</td></tr><tr><td>20210527-153624/validation/epoch_mean_squared_error</td><td>0.00875</td></tr><tr><td>20210527-153624/validation/epoch_root_mean_squared_error</td><td>0.09356</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-153624/train/global_step</td><td>▁▃▁▂▃▄▅▆▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁█▁████████████████████████████</td></tr><tr><td>global_step</td><td>▁▁▂▃▃▄▄▅▅▅▅▆▆▆▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▂▂▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▃▂▂▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/global_step</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>20210527-153624/validation/evaluation_loss_vs_iterations</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/train/epoch_loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153624/train/epoch_mean_absolute_error</td><td>█▃▂▂▁▁▁▁</td></tr><tr><td>20210527-153624/train/epoch_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153624/train/epoch_root_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153624/validation/epoch_loss</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/epoch_mean_absolute_error</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/epoch_mean_squared_error</td><td>█▆▃▂▂▁▁▁</td></tr><tr><td>20210527-153624/validation/epoch_root_mean_squared_error</td><td>█▆▃▂▂▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 9 media file(s), 1271 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastdense:single_in_step:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/jqmw4r1z\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/jqmw4r1z</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastdense:wide_in_step:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/nerlrr9h\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/nerlrr9h</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153731-nerlrr9h</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 3/45 [=>............................] - ETA: 2s - loss: 0.1052 - mean_absolute_error: 0.2740 - mean_squared_error: 0.1052 - root_mean_squared_error: 0.3244 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0208s). Check your callbacks.\n",
      "34/45 [=====================>........] - ETA: 0s - loss: 0.0407 - mean_absolute_error: 0.1507 - mean_squared_error: 0.0407 - root_mean_squared_error: 0.2017\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "45/45 [==============================] - 1s 11ms/step - loss: 0.0359 - mean_absolute_error: 0.1404 - mean_squared_error: 0.0359 - root_mean_squared_error: 0.1894 - val_loss: 0.0131 - val_mean_absolute_error: 0.0879 - val_mean_squared_error: 0.0131 - val_root_mean_squared_error: 0.1145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0168 - mean_absolute_error: 0.0982 - mean_squared_error: 0.0168 - root_mean_squared_error: 0.1297 - val_loss: 0.0121 - val_mean_absolute_error: 0.0856 - val_mean_squared_error: 0.0121 - val_root_mean_squared_error: 0.1102\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0155 - mean_absolute_error: 0.0940 - mean_squared_error: 0.0155 - root_mean_squared_error: 0.1244 - val_loss: 0.0119 - val_mean_absolute_error: 0.0853 - val_mean_squared_error: 0.0119 - val_root_mean_squared_error: 0.1089\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0148 - mean_absolute_error: 0.0916 - mean_squared_error: 0.0148 - root_mean_squared_error: 0.1216 - val_loss: 0.0114 - val_mean_absolute_error: 0.0831 - val_mean_squared_error: 0.0114 - val_root_mean_squared_error: 0.1068\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0144 - mean_absolute_error: 0.0902 - mean_squared_error: 0.0144 - root_mean_squared_error: 0.1200 - val_loss: 0.0112 - val_mean_absolute_error: 0.0823 - val_mean_squared_error: 0.0112 - val_root_mean_squared_error: 0.1060\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_absolute_error: 0.0893 - mean_squared_error: 0.0142 - root_mean_squared_error: 0.1190 - val_loss: 0.0111 - val_mean_absolute_error: 0.0818 - val_mean_squared_error: 0.0111 - val_root_mean_squared_error: 0.1055\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0140 - mean_absolute_error: 0.0887 - mean_squared_error: 0.0140 - root_mean_squared_error: 0.1184 - val_loss: 0.0111 - val_mean_absolute_error: 0.0815 - val_mean_squared_error: 0.0111 - val_root_mean_squared_error: 0.1052\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.0139 - mean_absolute_error: 0.0883 - mean_squared_error: 0.0139 - root_mean_squared_error: 0.1179 - val_loss: 0.0110 - val_mean_absolute_error: 0.0812 - val_mean_squared_error: 0.0110 - val_root_mean_squared_error: 0.1049\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastdense:wide_in_step/assets\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0110 - mean_absolute_error: 0.0812 - mean_squared_error: 0.0110 - root_mean_squared_error: 0.1049\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48303<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153731-nerlrr9h/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153731-nerlrr9h/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-153735/train/global_step</td><td>7</td></tr><tr><td>_timestamp</td><td>1622144300.73762</td></tr><tr><td>global_step</td><td>360</td></tr><tr><td>_step</td><td>32</td></tr><tr><td>_runtime</td><td>53</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>loss</td><td>0.01391</td></tr><tr><td>mean_absolute_error</td><td>0.0883</td></tr><tr><td>mean_squared_error</td><td>0.01391</td></tr><tr><td>root_mean_squared_error</td><td>0.11792</td></tr><tr><td>val_loss</td><td>0.01101</td></tr><tr><td>val_mean_absolute_error</td><td>0.08119</td></tr><tr><td>val_mean_squared_error</td><td>0.01101</td></tr><tr><td>val_root_mean_squared_error</td><td>0.10493</td></tr><tr><td>best_val_loss</td><td>0.01101</td></tr><tr><td>best_epoch</td><td>7</td></tr><tr><td>20210527-153735/validation/global_step</td><td>7</td></tr><tr><td>20210527-153735/validation/evaluation_loss_vs_iterations</td><td>0.01101</td></tr><tr><td>20210527-153735/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.08119</td></tr><tr><td>20210527-153735/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.01101</td></tr><tr><td>20210527-153735/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.10493</td></tr><tr><td>20210527-153735/train/epoch_loss</td><td>0.01391</td></tr><tr><td>20210527-153735/train/epoch_mean_absolute_error</td><td>0.0883</td></tr><tr><td>20210527-153735/train/epoch_mean_squared_error</td><td>0.01391</td></tr><tr><td>20210527-153735/train/epoch_root_mean_squared_error</td><td>0.11792</td></tr><tr><td>20210527-153735/validation/epoch_loss</td><td>0.01101</td></tr><tr><td>20210527-153735/validation/epoch_mean_absolute_error</td><td>0.08119</td></tr><tr><td>20210527-153735/validation/epoch_mean_squared_error</td><td>0.01101</td></tr><tr><td>20210527-153735/validation/epoch_root_mean_squared_error</td><td>0.10493</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-153735/train/global_step</td><td>▁▃▁▂▃▄▅▆▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▄▄▄▅▅▅▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▂▂▃▃▄▄▅▅▆▆▆▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▆▅▃▂▂▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/validation/global_step</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>20210527-153735/validation/evaluation_loss_vs_iterations</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▆▅▃▂▂▁▁</td></tr><tr><td>20210527-153735/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/train/epoch_loss</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153735/train/epoch_mean_absolute_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153735/train/epoch_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153735/train/epoch_root_mean_squared_error</td><td>█▂▂▁▁▁▁▁</td></tr><tr><td>20210527-153735/validation/epoch_loss</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/validation/epoch_mean_absolute_error</td><td>█▆▅▃▂▂▁▁</td></tr><tr><td>20210527-153735/validation/epoch_mean_squared_error</td><td>█▅▄▂▂▁▁▁</td></tr><tr><td>20210527-153735/validation/epoch_root_mean_squared_error</td><td>█▅▄▂▂▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 9 media file(s), 1279 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastdense:wide_in_step:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/nerlrr9h\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/nerlrr9h</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstm:wide_in_step:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/355necrn\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/355necrn</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153836-355necrn</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 5/45 [==>...........................] - ETA: 4s - loss: 1.3529 - mean_absolute_error: 0.9075 - mean_squared_error: 1.3529 - root_mean_squared_error: 1.1631WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0248s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
      "43/45 [===========================>..] - ETA: 0s - loss: 0.7472 - mean_absolute_error: 0.6556 - mean_squared_error: 0.7472 - root_mean_squared_error: 0.8644\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "45/45 [==============================] - 4s 42ms/step - loss: 0.7385 - mean_absolute_error: 0.6516 - mean_squared_error: 0.7390 - root_mean_squared_error: 0.8596 - val_loss: 0.0497 - val_mean_absolute_error: 0.1772 - val_mean_squared_error: 0.0496 - val_root_mean_squared_error: 0.2228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.3021 - mean_absolute_error: 0.4111 - mean_squared_error: 0.3023 - root_mean_squared_error: 0.5499 - val_loss: 0.0268 - val_mean_absolute_error: 0.1196 - val_mean_squared_error: 0.0268 - val_root_mean_squared_error: 0.1638\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.2077 - mean_absolute_error: 0.3295 - mean_squared_error: 0.2078 - root_mean_squared_error: 0.4559 - val_loss: 0.0234 - val_mean_absolute_error: 0.1153 - val_mean_squared_error: 0.0235 - val_root_mean_squared_error: 0.1532\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.1628 - mean_absolute_error: 0.2822 - mean_squared_error: 0.1629 - root_mean_squared_error: 0.4036 - val_loss: 0.0189 - val_mean_absolute_error: 0.1068 - val_mean_squared_error: 0.0189 - val_root_mean_squared_error: 0.1376\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.1331 - mean_absolute_error: 0.2530 - mean_squared_error: 0.1332 - root_mean_squared_error: 0.3650 - val_loss: 0.0162 - val_mean_absolute_error: 0.0990 - val_mean_squared_error: 0.0162 - val_root_mean_squared_error: 0.1274\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.1148 - mean_absolute_error: 0.2323 - mean_squared_error: 0.1149 - root_mean_squared_error: 0.3389 - val_loss: 0.0150 - val_mean_absolute_error: 0.0974 - val_mean_squared_error: 0.0150 - val_root_mean_squared_error: 0.1227\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.1003 - mean_absolute_error: 0.2164 - mean_squared_error: 0.1004 - root_mean_squared_error: 0.3168 - val_loss: 0.0151 - val_mean_absolute_error: 0.0968 - val_mean_squared_error: 0.0151 - val_root_mean_squared_error: 0.1231\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.0915 - mean_absolute_error: 0.2043 - mean_squared_error: 0.0915 - root_mean_squared_error: 0.3026 - val_loss: 0.0147 - val_mean_absolute_error: 0.0969 - val_mean_squared_error: 0.0147 - val_root_mean_squared_error: 0.1213\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0779 - mean_absolute_error: 0.1897 - mean_squared_error: 0.0780 - root_mean_squared_error: 0.2793 - val_loss: 0.0140 - val_mean_absolute_error: 0.0938 - val_mean_squared_error: 0.0140 - val_root_mean_squared_error: 0.1184\n",
      "End epoch 8 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0728 - mean_absolute_error: 0.1827 - mean_squared_error: 0.0728 - root_mean_squared_error: 0.2699 - val_loss: 0.0131 - val_mean_absolute_error: 0.0905 - val_mean_squared_error: 0.0131 - val_root_mean_squared_error: 0.1145\n",
      "End epoch 9 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 0.0657 - mean_absolute_error: 0.1737 - mean_squared_error: 0.0657 - root_mean_squared_error: 0.2564 - val_loss: 0.0132 - val_mean_absolute_error: 0.0915 - val_mean_squared_error: 0.0132 - val_root_mean_squared_error: 0.1150\n",
      "End epoch 10 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 0.0574 - mean_absolute_error: 0.1650 - mean_squared_error: 0.0574 - root_mean_squared_error: 0.2396 - val_loss: 0.0128 - val_mean_absolute_error: 0.0898 - val_mean_squared_error: 0.0128 - val_root_mean_squared_error: 0.1132\n",
      "End epoch 11 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0540 - mean_absolute_error: 0.1594 - mean_squared_error: 0.0540 - root_mean_squared_error: 0.2324 - val_loss: 0.0129 - val_mean_absolute_error: 0.0902 - val_mean_squared_error: 0.0130 - val_root_mean_squared_error: 0.1138\n",
      "End epoch 12 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0481 - mean_absolute_error: 0.1531 - mean_squared_error: 0.0482 - root_mean_squared_error: 0.2195 - val_loss: 0.0128 - val_mean_absolute_error: 0.0900 - val_mean_squared_error: 0.0129 - val_root_mean_squared_error: 0.1134\n",
      "End epoch 13 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0449 - mean_absolute_error: 0.1474 - mean_squared_error: 0.0450 - root_mean_squared_error: 0.2121 - val_loss: 0.0127 - val_mean_absolute_error: 0.0897 - val_mean_squared_error: 0.0128 - val_root_mean_squared_error: 0.1130\n",
      "End epoch 14 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0433 - mean_absolute_error: 0.1448 - mean_squared_error: 0.0434 - root_mean_squared_error: 0.2082 - val_loss: 0.0127 - val_mean_absolute_error: 0.0897 - val_mean_squared_error: 0.0128 - val_root_mean_squared_error: 0.1131\n",
      "End epoch 15 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 0.0383 - mean_absolute_error: 0.1386 - mean_squared_error: 0.0383 - root_mean_squared_error: 0.1958 - val_loss: 0.0127 - val_mean_absolute_error: 0.0896 - val_mean_squared_error: 0.0128 - val_root_mean_squared_error: 0.1131\n",
      "End epoch 16 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastlstm:wide_in_step/assets\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.0127 - mean_absolute_error: 0.0896 - mean_squared_error: 0.0128 - root_mean_squared_error: 0.1131\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48487<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153836-355necrn/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153836-355necrn/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>63</td></tr><tr><td>_timestamp</td><td>1622144378.47667</td></tr><tr><td>_step</td><td>68</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>loss</td><td>0.03831</td></tr><tr><td>mean_absolute_error</td><td>0.13863</td></tr><tr><td>mean_squared_error</td><td>0.03834</td></tr><tr><td>root_mean_squared_error</td><td>0.1958</td></tr><tr><td>val_loss</td><td>0.01275</td></tr><tr><td>val_mean_absolute_error</td><td>0.08958</td></tr><tr><td>val_mean_squared_error</td><td>0.01279</td></tr><tr><td>val_root_mean_squared_error</td><td>0.11309</td></tr><tr><td>best_val_loss</td><td>0.01274</td></tr><tr><td>best_epoch</td><td>14</td></tr><tr><td>20210527-153841/train/global_step</td><td>16</td></tr><tr><td>global_step</td><td>765</td></tr><tr><td>20210527-153841/validation/global_step</td><td>16</td></tr><tr><td>20210527-153841/validation/evaluation_loss_vs_iterations</td><td>0.01275</td></tr><tr><td>20210527-153841/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.08958</td></tr><tr><td>20210527-153841/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.01279</td></tr><tr><td>20210527-153841/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.11309</td></tr><tr><td>20210527-153841/train/epoch_loss</td><td>0.03831</td></tr><tr><td>20210527-153841/train/epoch_mean_absolute_error</td><td>0.13863</td></tr><tr><td>20210527-153841/train/epoch_mean_squared_error</td><td>0.03834</td></tr><tr><td>20210527-153841/train/epoch_root_mean_squared_error</td><td>0.1958</td></tr><tr><td>20210527-153841/validation/epoch_loss</td><td>0.01275</td></tr><tr><td>20210527-153841/validation/epoch_mean_absolute_error</td><td>0.08958</td></tr><tr><td>20210527-153841/validation/epoch_mean_squared_error</td><td>0.01279</td></tr><tr><td>20210527-153841/validation/epoch_root_mean_squared_error</td><td>0.11309</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/train/global_step</td><td>▁▂▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>20210527-153841/validation/global_step</td><td>▁█▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>20210527-153841/validation/evaluation_loss_vs_iterations</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/train/epoch_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/train/epoch_mean_absolute_error</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/train/epoch_mean_squared_error</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/train/epoch_root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/epoch_loss</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/epoch_mean_absolute_error</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/epoch_mean_squared_error</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-153841/validation/epoch_root_mean_squared_error</td><td>█▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 18 media file(s), 34 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstm:wide_in_step:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/355necrn\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/355necrn</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastbilstm:wide_in_step:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/34zex2t5\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/34zex2t5</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153956-34zex2t5</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 6/45 [===>..........................] - ETA: 5s - loss: 1.3529 - mean_absolute_error: 0.9154 - mean_squared_error: 1.3529 - root_mean_squared_error: 1.1632 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0178s vs `on_train_batch_end` time: 0.1018s). Check your callbacks.\n",
      "44/45 [============================>.] - ETA: 0s - loss: 0.7731 - mean_absolute_error: 0.6793 - mean_squared_error: 0.7731 - root_mean_squared_error: 0.8792\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "45/45 [==============================] - 6s 41ms/step - loss: 0.7725 - mean_absolute_error: 0.6793 - mean_squared_error: 0.7730 - root_mean_squared_error: 0.8792 - val_loss: 0.0720 - val_mean_absolute_error: 0.2211 - val_mean_squared_error: 0.0719 - val_root_mean_squared_error: 0.2682\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.3179 - mean_absolute_error: 0.4394 - mean_squared_error: 0.3181 - root_mean_squared_error: 0.5640 - val_loss: 0.0685 - val_mean_absolute_error: 0.2114 - val_mean_squared_error: 0.0684 - val_root_mean_squared_error: 0.2616\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.2112 - mean_absolute_error: 0.3531 - mean_squared_error: 0.2114 - root_mean_squared_error: 0.4598 - val_loss: 0.0483 - val_mean_absolute_error: 0.1793 - val_mean_squared_error: 0.0483 - val_root_mean_squared_error: 0.2198\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1593 - mean_absolute_error: 0.3053 - mean_squared_error: 0.1594 - root_mean_squared_error: 0.3993 - val_loss: 0.0361 - val_mean_absolute_error: 0.1555 - val_mean_squared_error: 0.0361 - val_root_mean_squared_error: 0.1900\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.1319 - mean_absolute_error: 0.2755 - mean_squared_error: 0.1320 - root_mean_squared_error: 0.3633 - val_loss: 0.0278 - val_mean_absolute_error: 0.1353 - val_mean_squared_error: 0.0278 - val_root_mean_squared_error: 0.1667\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.1120 - mean_absolute_error: 0.2536 - mean_squared_error: 0.1121 - root_mean_squared_error: 0.3348 - val_loss: 0.0203 - val_mean_absolute_error: 0.1149 - val_mean_squared_error: 0.0203 - val_root_mean_squared_error: 0.1425\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0974 - mean_absolute_error: 0.2346 - mean_squared_error: 0.0974 - root_mean_squared_error: 0.3121 - val_loss: 0.0176 - val_mean_absolute_error: 0.1064 - val_mean_squared_error: 0.0177 - val_root_mean_squared_error: 0.1330\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0834 - mean_absolute_error: 0.2168 - mean_squared_error: 0.0835 - root_mean_squared_error: 0.2889 - val_loss: 0.0150 - val_mean_absolute_error: 0.0968 - val_mean_squared_error: 0.0151 - val_root_mean_squared_error: 0.1228\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0745 - mean_absolute_error: 0.2057 - mean_squared_error: 0.0745 - root_mean_squared_error: 0.2730 - val_loss: 0.0148 - val_mean_absolute_error: 0.0967 - val_mean_squared_error: 0.0149 - val_root_mean_squared_error: 0.1220\n",
      "End epoch 8 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 1s 12ms/step - loss: 0.0662 - mean_absolute_error: 0.1941 - mean_squared_error: 0.0662 - root_mean_squared_error: 0.2573 - val_loss: 0.0140 - val_mean_absolute_error: 0.0941 - val_mean_squared_error: 0.0141 - val_root_mean_squared_error: 0.1186\n",
      "End epoch 9 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0585 - mean_absolute_error: 0.1824 - mean_squared_error: 0.0585 - root_mean_squared_error: 0.2420 - val_loss: 0.0139 - val_mean_absolute_error: 0.0933 - val_mean_squared_error: 0.0139 - val_root_mean_squared_error: 0.1180\n",
      "End epoch 10 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0523 - mean_absolute_error: 0.1726 - mean_squared_error: 0.0524 - root_mean_squared_error: 0.2288 - val_loss: 0.0135 - val_mean_absolute_error: 0.0922 - val_mean_squared_error: 0.0135 - val_root_mean_squared_error: 0.1163\n",
      "End epoch 11 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0481 - mean_absolute_error: 0.1664 - mean_squared_error: 0.0481 - root_mean_squared_error: 0.2194 - val_loss: 0.0134 - val_mean_absolute_error: 0.0919 - val_mean_squared_error: 0.0134 - val_root_mean_squared_error: 0.1160\n",
      "End epoch 12 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0439 - mean_absolute_error: 0.1582 - mean_squared_error: 0.0439 - root_mean_squared_error: 0.2095 - val_loss: 0.0129 - val_mean_absolute_error: 0.0901 - val_mean_squared_error: 0.0130 - val_root_mean_squared_error: 0.1139\n",
      "End epoch 13 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0381 - mean_absolute_error: 0.1480 - mean_squared_error: 0.0381 - root_mean_squared_error: 0.1951 - val_loss: 0.0127 - val_mean_absolute_error: 0.0899 - val_mean_squared_error: 0.0127 - val_root_mean_squared_error: 0.1128\n",
      "End epoch 14 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 16/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0362 - mean_absolute_error: 0.1441 - mean_squared_error: 0.0362 - root_mean_squared_error: 0.1903 - val_loss: 0.0126 - val_mean_absolute_error: 0.0887 - val_mean_squared_error: 0.0126 - val_root_mean_squared_error: 0.1123\n",
      "End epoch 15 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 17/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0339 - mean_absolute_error: 0.1392 - mean_squared_error: 0.0339 - root_mean_squared_error: 0.1840 - val_loss: 0.0130 - val_mean_absolute_error: 0.0905 - val_mean_squared_error: 0.0130 - val_root_mean_squared_error: 0.1140\n",
      "End epoch 16 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 18/30\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.0313 - mean_absolute_error: 0.1344 - mean_squared_error: 0.0313 - root_mean_squared_error: 0.1770 - val_loss: 0.0133 - val_mean_absolute_error: 0.0917 - val_mean_squared_error: 0.0133 - val_root_mean_squared_error: 0.1153\n",
      "End epoch 17 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 19/30\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.0299 - mean_absolute_error: 0.1314 - mean_squared_error: 0.0299 - root_mean_squared_error: 0.1729 - val_loss: 0.0131 - val_mean_absolute_error: 0.0912 - val_mean_squared_error: 0.0131 - val_root_mean_squared_error: 0.1145\n",
      "End epoch 18 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastbilstm:wide_in_step/assets\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0131 - mean_absolute_error: 0.0912 - mean_squared_error: 0.0131 - root_mean_squared_error: 0.1145\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48715<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153956-34zex2t5/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_153956-34zex2t5/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154001/train/global_step</td><td>18</td></tr><tr><td>_timestamp</td><td>1622144462.18469</td></tr><tr><td>global_step</td><td>855</td></tr><tr><td>_step</td><td>76</td></tr><tr><td>_runtime</td><td>68</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>loss</td><td>0.02988</td></tr><tr><td>mean_absolute_error</td><td>0.13143</td></tr><tr><td>mean_squared_error</td><td>0.02989</td></tr><tr><td>root_mean_squared_error</td><td>0.17288</td></tr><tr><td>val_loss</td><td>0.01307</td></tr><tr><td>val_mean_absolute_error</td><td>0.09123</td></tr><tr><td>val_mean_squared_error</td><td>0.0131</td></tr><tr><td>val_root_mean_squared_error</td><td>0.11447</td></tr><tr><td>best_val_loss</td><td>0.01257</td></tr><tr><td>best_epoch</td><td>15</td></tr><tr><td>20210527-154001/validation/global_step</td><td>18</td></tr><tr><td>20210527-154001/validation/evaluation_loss_vs_iterations</td><td>0.01307</td></tr><tr><td>20210527-154001/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.09123</td></tr><tr><td>20210527-154001/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.0131</td></tr><tr><td>20210527-154001/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.11447</td></tr><tr><td>20210527-154001/train/epoch_loss</td><td>0.02988</td></tr><tr><td>20210527-154001/train/epoch_mean_absolute_error</td><td>0.13143</td></tr><tr><td>20210527-154001/train/epoch_mean_squared_error</td><td>0.02989</td></tr><tr><td>20210527-154001/train/epoch_root_mean_squared_error</td><td>0.17288</td></tr><tr><td>20210527-154001/validation/epoch_loss</td><td>0.01307</td></tr><tr><td>20210527-154001/validation/epoch_mean_absolute_error</td><td>0.09123</td></tr><tr><td>20210527-154001/validation/epoch_mean_squared_error</td><td>0.0131</td></tr><tr><td>20210527-154001/validation/epoch_root_mean_squared_error</td><td>0.11447</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154001/train/global_step</td><td>▁▂▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▇▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>██▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>20210527-154001/validation/evaluation_loss_vs_iterations</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▇▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/evaluation_mean_squared_error_vs_iterations</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>██▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/train/epoch_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/train/epoch_mean_absolute_error</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/train/epoch_mean_squared_error</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/train/epoch_root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/epoch_loss</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/epoch_mean_absolute_error</td><td>█▇▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/epoch_mean_squared_error</td><td>██▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154001/validation/epoch_root_mean_squared_error</td><td>██▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 20 media file(s), 38 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastbilstm:wide_in_step:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/34zex2t5\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/34zex2t5</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/3628rxqv\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/3628rxqv</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154122-3628rxqv</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 5/45 [==>...........................] - ETA: 6s - loss: 1.3794 - mean_absolute_error: 0.9279 - mean_squared_error: 1.3794 - root_mean_squared_error: 1.1745 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0394s vs `on_train_batch_end` time: 0.0774s). Check your callbacks.\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.6993 - mean_absolute_error: 0.6387 - mean_squared_error: 0.6993 - root_mean_squared_error: 0.8362\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "45/45 [==============================] - 5s 56ms/step - loss: 0.6993 - mean_absolute_error: 0.6387 - mean_squared_error: 0.6993 - root_mean_squared_error: 0.8362 - val_loss: 0.0677 - val_mean_absolute_error: 0.2094 - val_mean_squared_error: 0.0677 - val_root_mean_squared_error: 0.2603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.3348 - mean_absolute_error: 0.4360 - mean_squared_error: 0.3348 - root_mean_squared_error: 0.5786 - val_loss: 0.0309 - val_mean_absolute_error: 0.1404 - val_mean_squared_error: 0.0309 - val_root_mean_squared_error: 0.1757\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.2442 - mean_absolute_error: 0.3629 - mean_squared_error: 0.2442 - root_mean_squared_error: 0.4941 - val_loss: 0.0224 - val_mean_absolute_error: 0.1175 - val_mean_squared_error: 0.0224 - val_root_mean_squared_error: 0.1497\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.1962 - mean_absolute_error: 0.3167 - mean_squared_error: 0.1962 - root_mean_squared_error: 0.4430 - val_loss: 0.0154 - val_mean_absolute_error: 0.0973 - val_mean_squared_error: 0.0154 - val_root_mean_squared_error: 0.1239\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.1592 - mean_absolute_error: 0.2818 - mean_squared_error: 0.1592 - root_mean_squared_error: 0.3990 - val_loss: 0.0156 - val_mean_absolute_error: 0.0992 - val_mean_squared_error: 0.0156 - val_root_mean_squared_error: 0.1249\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.1286 - mean_absolute_error: 0.2513 - mean_squared_error: 0.1286 - root_mean_squared_error: 0.3585 - val_loss: 0.0138 - val_mean_absolute_error: 0.0938 - val_mean_squared_error: 0.0138 - val_root_mean_squared_error: 0.1177\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.1066 - mean_absolute_error: 0.2269 - mean_squared_error: 0.1066 - root_mean_squared_error: 0.3265 - val_loss: 0.0124 - val_mean_absolute_error: 0.0870 - val_mean_squared_error: 0.0124 - val_root_mean_squared_error: 0.1115\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.0881 - mean_absolute_error: 0.2061 - mean_squared_error: 0.0881 - root_mean_squared_error: 0.2968 - val_loss: 0.0124 - val_mean_absolute_error: 0.0875 - val_mean_squared_error: 0.0124 - val_root_mean_squared_error: 0.1115\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 9/30\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 0.0747 - mean_absolute_error: 0.1906 - mean_squared_error: 0.0747 - root_mean_squared_error: 0.2733 - val_loss: 0.0126 - val_mean_absolute_error: 0.0894 - val_mean_squared_error: 0.0126 - val_root_mean_squared_error: 0.1123\n",
      "End epoch 8 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 10/30\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.0649 - mean_absolute_error: 0.1775 - mean_squared_error: 0.0649 - root_mean_squared_error: 0.2547 - val_loss: 0.0112 - val_mean_absolute_error: 0.0830 - val_mean_squared_error: 0.0112 - val_root_mean_squared_error: 0.1060\n",
      "End epoch 9 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 11/30\n",
      "45/45 [==============================] - 2s 34ms/step - loss: 0.0553 - mean_absolute_error: 0.1656 - mean_squared_error: 0.0553 - root_mean_squared_error: 0.2351 - val_loss: 0.0110 - val_mean_absolute_error: 0.0815 - val_mean_squared_error: 0.0110 - val_root_mean_squared_error: 0.1048\n",
      "End epoch 10 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 12/30\n",
      "45/45 [==============================] - 2s 35ms/step - loss: 0.0479 - mean_absolute_error: 0.1545 - mean_squared_error: 0.0479 - root_mean_squared_error: 0.2188 - val_loss: 0.0109 - val_mean_absolute_error: 0.0809 - val_mean_squared_error: 0.0109 - val_root_mean_squared_error: 0.1045\n",
      "End epoch 11 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 13/30\n",
      "45/45 [==============================] - 1s 32ms/step - loss: 0.0412 - mean_absolute_error: 0.1442 - mean_squared_error: 0.0412 - root_mean_squared_error: 0.2030 - val_loss: 0.0110 - val_mean_absolute_error: 0.0811 - val_mean_squared_error: 0.0110 - val_root_mean_squared_error: 0.1050\n",
      "End epoch 12 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 14/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.0365 - mean_absolute_error: 0.1366 - mean_squared_error: 0.0365 - root_mean_squared_error: 0.1911 - val_loss: 0.0107 - val_mean_absolute_error: 0.0793 - val_mean_squared_error: 0.0107 - val_root_mean_squared_error: 0.1034\n",
      "End epoch 13 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 15/30\n",
      "45/45 [==============================] - 1s 33ms/step - loss: 0.0321 - mean_absolute_error: 0.1288 - mean_squared_error: 0.0321 - root_mean_squared_error: 0.1793 - val_loss: 0.0108 - val_mean_absolute_error: 0.0783 - val_mean_squared_error: 0.0108 - val_root_mean_squared_error: 0.1037\n",
      "End epoch 14 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastlstmoneshot:wide_in_out/assets\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0108 - mean_absolute_error: 0.0783 - mean_squared_error: 0.0108 - root_mean_squared_error: 0.1037\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 48946<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154122-3628rxqv/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154122-3628rxqv/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154125/train/global_step</td><td>14</td></tr><tr><td>_timestamp</td><td>1622144585.59007</td></tr><tr><td>global_step</td><td>675</td></tr><tr><td>_step</td><td>60</td></tr><tr><td>_runtime</td><td>108</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>loss</td><td>0.03214</td></tr><tr><td>mean_absolute_error</td><td>0.1288</td></tr><tr><td>mean_squared_error</td><td>0.03214</td></tr><tr><td>root_mean_squared_error</td><td>0.17928</td></tr><tr><td>val_loss</td><td>0.01076</td></tr><tr><td>val_mean_absolute_error</td><td>0.07829</td></tr><tr><td>val_mean_squared_error</td><td>0.01076</td></tr><tr><td>val_root_mean_squared_error</td><td>0.10372</td></tr><tr><td>best_val_loss</td><td>0.01069</td></tr><tr><td>best_epoch</td><td>13</td></tr><tr><td>20210527-154125/validation/global_step</td><td>14</td></tr><tr><td>20210527-154125/validation/evaluation_loss_vs_iterations</td><td>0.01076</td></tr><tr><td>20210527-154125/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.07829</td></tr><tr><td>20210527-154125/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.01076</td></tr><tr><td>20210527-154125/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.10372</td></tr><tr><td>20210527-154125/train/epoch_loss</td><td>0.03214</td></tr><tr><td>20210527-154125/train/epoch_mean_absolute_error</td><td>0.1288</td></tr><tr><td>20210527-154125/train/epoch_mean_squared_error</td><td>0.03214</td></tr><tr><td>20210527-154125/train/epoch_root_mean_squared_error</td><td>0.17928</td></tr><tr><td>20210527-154125/validation/epoch_loss</td><td>0.01076</td></tr><tr><td>20210527-154125/validation/epoch_mean_absolute_error</td><td>0.07829</td></tr><tr><td>20210527-154125/validation/epoch_mean_squared_error</td><td>0.01076</td></tr><tr><td>20210527-154125/validation/epoch_root_mean_squared_error</td><td>0.10372</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154125/train/global_step</td><td>▁▂▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▅▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▄▃▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/global_step</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>20210527-154125/validation/evaluation_loss_vs_iterations</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▄▃▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/train/epoch_loss</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/train/epoch_mean_absolute_error</td><td>█▅▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>20210527-154125/train/epoch_mean_squared_error</td><td>█▄▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/train/epoch_root_mean_squared_error</td><td>█▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>20210527-154125/validation/epoch_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/epoch_mean_absolute_error</td><td>█▄▃▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/epoch_mean_squared_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154125/validation/epoch_root_mean_squared_error</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 16 media file(s), 2175 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/3628rxqv\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/3628rxqv</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/394oai8h\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/394oai8h</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154331-394oai8h</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 6/44 [===>..........................] - ETA: 4s - loss: 1.3175 - mean_absolute_error: 0.9014 - mean_squared_error: 1.3175 - root_mean_squared_error: 1.1478WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0399s vs `on_train_batch_end` time: 0.0678s). Check your callbacks.\n",
      "44/44 [==============================] - ETA: 0s - loss: 0.6973 - mean_absolute_error: 0.6335 - mean_squared_error: 0.6973 - root_mean_squared_error: 0.8351\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "44/44 [==============================] - 5s 54ms/step - loss: 0.6973 - mean_absolute_error: 0.6335 - mean_squared_error: 0.6973 - root_mean_squared_error: 0.8351 - val_loss: 0.0605 - val_mean_absolute_error: 0.1968 - val_mean_squared_error: 0.0605 - val_root_mean_squared_error: 0.2460\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.3375 - mean_absolute_error: 0.4238 - mean_squared_error: 0.3375 - root_mean_squared_error: 0.5810 - val_loss: 0.0234 - val_mean_absolute_error: 0.1191 - val_mean_squared_error: 0.0234 - val_root_mean_squared_error: 0.1529\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.2450 - mean_absolute_error: 0.3449 - mean_squared_error: 0.2450 - root_mean_squared_error: 0.4950 - val_loss: 0.0161 - val_mean_absolute_error: 0.0994 - val_mean_squared_error: 0.0161 - val_root_mean_squared_error: 0.1268\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.2008 - mean_absolute_error: 0.3065 - mean_squared_error: 0.2008 - root_mean_squared_error: 0.4481 - val_loss: 0.0140 - val_mean_absolute_error: 0.0928 - val_mean_squared_error: 0.0140 - val_root_mean_squared_error: 0.1182\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.1654 - mean_absolute_error: 0.2759 - mean_squared_error: 0.1654 - root_mean_squared_error: 0.4067 - val_loss: 0.0127 - val_mean_absolute_error: 0.0877 - val_mean_squared_error: 0.0127 - val_root_mean_squared_error: 0.1125\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.1374 - mean_absolute_error: 0.2498 - mean_squared_error: 0.1374 - root_mean_squared_error: 0.3707 - val_loss: 0.0121 - val_mean_absolute_error: 0.0855 - val_mean_squared_error: 0.0121 - val_root_mean_squared_error: 0.1102\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.1167 - mean_absolute_error: 0.2293 - mean_squared_error: 0.1167 - root_mean_squared_error: 0.3416 - val_loss: 0.0121 - val_mean_absolute_error: 0.0859 - val_mean_squared_error: 0.0121 - val_root_mean_squared_error: 0.1101\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.0995 - mean_absolute_error: 0.2123 - mean_squared_error: 0.0995 - root_mean_squared_error: 0.3155 - val_loss: 0.0120 - val_mean_absolute_error: 0.0859 - val_mean_squared_error: 0.0120 - val_root_mean_squared_error: 0.1097\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0843 - mean_absolute_error: 0.1950 - mean_squared_error: 0.0843 - root_mean_squared_error: 0.2903 - val_loss: 0.0119 - val_mean_absolute_error: 0.0854 - val_mean_squared_error: 0.0119 - val_root_mean_squared_error: 0.1090\n",
      "End epoch 8 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.0739 - mean_absolute_error: 0.1838 - mean_squared_error: 0.0739 - root_mean_squared_error: 0.2718 - val_loss: 0.0115 - val_mean_absolute_error: 0.0834 - val_mean_squared_error: 0.0115 - val_root_mean_squared_error: 0.1073\n",
      "End epoch 9 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.0642 - mean_absolute_error: 0.1717 - mean_squared_error: 0.0642 - root_mean_squared_error: 0.2534 - val_loss: 0.0113 - val_mean_absolute_error: 0.0829 - val_mean_squared_error: 0.0113 - val_root_mean_squared_error: 0.1062\n",
      "End epoch 10 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0561 - mean_absolute_error: 0.1615 - mean_squared_error: 0.0561 - root_mean_squared_error: 0.2369 - val_loss: 0.0109 - val_mean_absolute_error: 0.0806 - val_mean_squared_error: 0.0109 - val_root_mean_squared_error: 0.1045\n",
      "End epoch 11 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 13/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0489 - mean_absolute_error: 0.1524 - mean_squared_error: 0.0489 - root_mean_squared_error: 0.2212 - val_loss: 0.0106 - val_mean_absolute_error: 0.0800 - val_mean_squared_error: 0.0106 - val_root_mean_squared_error: 0.1031\n",
      "End epoch 12 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 14/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.0433 - mean_absolute_error: 0.1441 - mean_squared_error: 0.0433 - root_mean_squared_error: 0.2080 - val_loss: 0.0105 - val_mean_absolute_error: 0.0779 - val_mean_squared_error: 0.0105 - val_root_mean_squared_error: 0.1023\n",
      "End epoch 13 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 15/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0386 - mean_absolute_error: 0.1375 - mean_squared_error: 0.0386 - root_mean_squared_error: 0.1964 - val_loss: 0.0106 - val_mean_absolute_error: 0.0786 - val_mean_squared_error: 0.0106 - val_root_mean_squared_error: 0.1028\n",
      "End epoch 14 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 16/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.0345 - mean_absolute_error: 0.1308 - mean_squared_error: 0.0345 - root_mean_squared_error: 0.1858 - val_loss: 0.0104 - val_mean_absolute_error: 0.0777 - val_mean_squared_error: 0.0104 - val_root_mean_squared_error: 0.1021\n",
      "End epoch 15 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 17/30\n",
      "44/44 [==============================] - 2s 34ms/step - loss: 0.0310 - mean_absolute_error: 0.1248 - mean_squared_error: 0.0310 - root_mean_squared_error: 0.1761 - val_loss: 0.0102 - val_mean_absolute_error: 0.0761 - val_mean_squared_error: 0.0102 - val_root_mean_squared_error: 0.1009\n",
      "End epoch 16 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 18/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.0280 - mean_absolute_error: 0.1197 - mean_squared_error: 0.0280 - root_mean_squared_error: 0.1672 - val_loss: 0.0102 - val_mean_absolute_error: 0.0760 - val_mean_squared_error: 0.0102 - val_root_mean_squared_error: 0.1009\n",
      "End epoch 17 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 19/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0254 - mean_absolute_error: 0.1146 - mean_squared_error: 0.0254 - root_mean_squared_error: 0.1594 - val_loss: 0.0098 - val_mean_absolute_error: 0.0744 - val_mean_squared_error: 0.0098 - val_root_mean_squared_error: 0.0988\n",
      "End epoch 18 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk/assets\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.0098 - mean_absolute_error: 0.0744 - mean_squared_error: 0.0098 - root_mean_squared_error: 0.0988\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 49272<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154331-394oai8h/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154331-394oai8h/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154334/train/global_step</td><td>18</td></tr><tr><td>_timestamp</td><td>1622144736.95765</td></tr><tr><td>global_step</td><td>836</td></tr><tr><td>_step</td><td>76</td></tr><tr><td>_runtime</td><td>133</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>loss</td><td>0.0254</td></tr><tr><td>mean_absolute_error</td><td>0.11464</td></tr><tr><td>mean_squared_error</td><td>0.0254</td></tr><tr><td>root_mean_squared_error</td><td>0.15936</td></tr><tr><td>val_loss</td><td>0.00977</td></tr><tr><td>val_mean_absolute_error</td><td>0.07445</td></tr><tr><td>val_mean_squared_error</td><td>0.00977</td></tr><tr><td>val_root_mean_squared_error</td><td>0.09884</td></tr><tr><td>best_val_loss</td><td>0.00977</td></tr><tr><td>best_epoch</td><td>18</td></tr><tr><td>20210527-154334/validation/global_step</td><td>18</td></tr><tr><td>20210527-154334/validation/evaluation_loss_vs_iterations</td><td>0.00977</td></tr><tr><td>20210527-154334/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.07445</td></tr><tr><td>20210527-154334/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.00977</td></tr><tr><td>20210527-154334/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.09884</td></tr><tr><td>20210527-154334/train/epoch_loss</td><td>0.0254</td></tr><tr><td>20210527-154334/train/epoch_mean_absolute_error</td><td>0.11464</td></tr><tr><td>20210527-154334/train/epoch_mean_squared_error</td><td>0.0254</td></tr><tr><td>20210527-154334/train/epoch_root_mean_squared_error</td><td>0.15936</td></tr><tr><td>20210527-154334/validation/epoch_loss</td><td>0.00977</td></tr><tr><td>20210527-154334/validation/epoch_mean_absolute_error</td><td>0.07445</td></tr><tr><td>20210527-154334/validation/epoch_mean_squared_error</td><td>0.00977</td></tr><tr><td>20210527-154334/validation/epoch_root_mean_squared_error</td><td>0.09884</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154334/train/global_step</td><td>▁▂▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>20210527-154334/validation/evaluation_loss_vs_iterations</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/train/epoch_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/train/epoch_mean_absolute_error</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/train/epoch_mean_squared_error</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/train/epoch_root_mean_squared_error</td><td>█▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/epoch_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/epoch_mean_absolute_error</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/epoch_mean_squared_error</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154334/validation/epoch_root_mean_squared_error</td><td>█▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 20 media file(s), 2623 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/394oai8h\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/394oai8h</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: Distributing <class 'numpy.ndarray'> object. This may take some time.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out_oneshot_hz:ops:3382hvgb</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/2thag039\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/2thag039</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154603-2thag039</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "Epoch 1/30\n",
      " 5/44 [==>...........................] - ETA: 5s - loss: 1.4226 - mean_absolute_error: 0.9395 - mean_squared_error: 1.4226 - root_mean_squared_error: 1.1927 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0365s vs `on_train_batch_end` time: 0.0712s). Check your callbacks.\n",
      "43/44 [============================>.] - ETA: 0s - loss: 0.7203 - mean_absolute_error: 0.6478 - mean_squared_error: 0.7203 - root_mean_squared_error: 0.8487\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n",
      "44/44 [==============================] - 5s 56ms/step - loss: 0.7194 - mean_absolute_error: 0.6475 - mean_squared_error: 0.7194 - root_mean_squared_error: 0.8482 - val_loss: 0.0583 - val_mean_absolute_error: 0.1955 - val_mean_squared_error: 0.0583 - val_root_mean_squared_error: 0.2416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Can't save model, h5py returned error: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 2/30\n",
      "44/44 [==============================] - 2s 37ms/step - loss: 0.3425 - mean_absolute_error: 0.4427 - mean_squared_error: 0.3425 - root_mean_squared_error: 0.5852 - val_loss: 0.0214 - val_mean_absolute_error: 0.1160 - val_mean_squared_error: 0.0214 - val_root_mean_squared_error: 0.1464\n",
      "End epoch 1 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 3/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.2483 - mean_absolute_error: 0.3680 - mean_squared_error: 0.2483 - root_mean_squared_error: 0.4983 - val_loss: 0.0158 - val_mean_absolute_error: 0.1009 - val_mean_squared_error: 0.0158 - val_root_mean_squared_error: 0.1257\n",
      "End epoch 2 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 4/30\n",
      "44/44 [==============================] - 1s 32ms/step - loss: 0.1977 - mean_absolute_error: 0.3223 - mean_squared_error: 0.1977 - root_mean_squared_error: 0.4446 - val_loss: 0.0144 - val_mean_absolute_error: 0.0959 - val_mean_squared_error: 0.0144 - val_root_mean_squared_error: 0.1200\n",
      "End epoch 3 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 5/30\n",
      "44/44 [==============================] - 2s 35ms/step - loss: 0.1635 - mean_absolute_error: 0.2883 - mean_squared_error: 0.1635 - root_mean_squared_error: 0.4043 - val_loss: 0.0122 - val_mean_absolute_error: 0.0876 - val_mean_squared_error: 0.0122 - val_root_mean_squared_error: 0.1107\n",
      "End epoch 4 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 6/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.1333 - mean_absolute_error: 0.2595 - mean_squared_error: 0.1333 - root_mean_squared_error: 0.3651 - val_loss: 0.0115 - val_mean_absolute_error: 0.0832 - val_mean_squared_error: 0.0115 - val_root_mean_squared_error: 0.1071\n",
      "End epoch 5 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 7/30\n",
      "44/44 [==============================] - 2s 36ms/step - loss: 0.1130 - mean_absolute_error: 0.2382 - mean_squared_error: 0.1130 - root_mean_squared_error: 0.3361 - val_loss: 0.0108 - val_mean_absolute_error: 0.0806 - val_mean_squared_error: 0.0108 - val_root_mean_squared_error: 0.1040\n",
      "End epoch 6 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 8/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0955 - mean_absolute_error: 0.2196 - mean_squared_error: 0.0955 - root_mean_squared_error: 0.3091 - val_loss: 0.0107 - val_mean_absolute_error: 0.0802 - val_mean_squared_error: 0.0107 - val_root_mean_squared_error: 0.1034\n",
      "End epoch 7 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 9/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.0789 - mean_absolute_error: 0.2001 - mean_squared_error: 0.0789 - root_mean_squared_error: 0.2809 - val_loss: 0.0103 - val_mean_absolute_error: 0.0780 - val_mean_squared_error: 0.0103 - val_root_mean_squared_error: 0.1015\n",
      "End epoch 8 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 10/30\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.0690 - mean_absolute_error: 0.1870 - mean_squared_error: 0.0690 - root_mean_squared_error: 0.2626 - val_loss: 0.0100 - val_mean_absolute_error: 0.0769 - val_mean_squared_error: 0.0100 - val_root_mean_squared_error: 0.1002\n",
      "End epoch 9 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 11/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0590 - mean_absolute_error: 0.1739 - mean_squared_error: 0.0590 - root_mean_squared_error: 0.2428 - val_loss: 0.0102 - val_mean_absolute_error: 0.0778 - val_mean_squared_error: 0.0102 - val_root_mean_squared_error: 0.1011\n",
      "End epoch 10 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "Epoch 12/30\n",
      "44/44 [==============================] - 1s 34ms/step - loss: 0.0513 - mean_absolute_error: 0.1628 - mean_squared_error: 0.0513 - root_mean_squared_error: 0.2266 - val_loss: 0.0105 - val_mean_absolute_error: 0.0787 - val_mean_squared_error: 0.0105 - val_root_mean_squared_error: 0.1024\n",
      "End epoch 11 of training; got log keys: ['loss', 'mean_absolute_error', 'mean_squared_error', 'root_mean_squared_error', 'val_loss', 'val_mean_absolute_error', 'val_mean_squared_error', 'val_root_mean_squared_error']\n",
      "INFO:tensorflow:Assets written to: experiments/snapshots/m5-fcst-base/exports/artifacts/models/modelforecastlstmoneshot:wide_in_out_oneshot_hz/assets\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0105 - mean_absolute_error: 0.0787 - mean_squared_error: 0.0105 - root_mean_squared_error: 0.1024\n",
      "UserWarning: `from_dict` defaulting to pandas implementation.\n",
      "UserWarning: Due to https://github.com/pandas-dev/pandas/issues/35092, Pandas ignores sort=False; Modin correctly does not sort.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 49666<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154603-2thag039/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_154603-2thag039/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154608/train/global_step</td><td>11</td></tr><tr><td>_timestamp</td><td>1622144840.81963</td></tr><tr><td>global_step</td><td>528</td></tr><tr><td>_step</td><td>48</td></tr><tr><td>_runtime</td><td>82</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>loss</td><td>0.05133</td></tr><tr><td>mean_absolute_error</td><td>0.16276</td></tr><tr><td>mean_squared_error</td><td>0.05133</td></tr><tr><td>root_mean_squared_error</td><td>0.22657</td></tr><tr><td>val_loss</td><td>0.01049</td></tr><tr><td>val_mean_absolute_error</td><td>0.07868</td></tr><tr><td>val_mean_squared_error</td><td>0.01049</td></tr><tr><td>val_root_mean_squared_error</td><td>0.1024</td></tr><tr><td>best_val_loss</td><td>0.01003</td></tr><tr><td>best_epoch</td><td>9</td></tr><tr><td>20210527-154608/validation/global_step</td><td>11</td></tr><tr><td>20210527-154608/validation/evaluation_loss_vs_iterations</td><td>0.01049</td></tr><tr><td>20210527-154608/validation/evaluation_mean_absolute_error_vs_iterations</td><td>0.07868</td></tr><tr><td>20210527-154608/validation/evaluation_mean_squared_error_vs_iterations</td><td>0.01049</td></tr><tr><td>20210527-154608/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>0.1024</td></tr><tr><td>20210527-154608/train/epoch_loss</td><td>0.05133</td></tr><tr><td>20210527-154608/train/epoch_mean_absolute_error</td><td>0.16276</td></tr><tr><td>20210527-154608/train/epoch_mean_squared_error</td><td>0.05133</td></tr><tr><td>20210527-154608/train/epoch_root_mean_squared_error</td><td>0.22657</td></tr><tr><td>20210527-154608/validation/epoch_loss</td><td>0.01049</td></tr><tr><td>20210527-154608/validation/epoch_mean_absolute_error</td><td>0.07868</td></tr><tr><td>20210527-154608/validation/epoch_mean_squared_error</td><td>0.01049</td></tr><tr><td>20210527-154608/validation/epoch_root_mean_squared_error</td><td>0.1024</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>20210527-154608/train/global_step</td><td>▁▂▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁██████</td></tr><tr><td>global_step</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇█</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>mean_absolute_error</td><td>█▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>mean_squared_error</td><td>█▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_absolute_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_mean_squared_error</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_root_mean_squared_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/global_step</td><td>▁▁█▁▁▁▁▁▁▁▁▂▂</td></tr><tr><td>20210527-154608/validation/evaluation_loss_vs_iterations</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/evaluation_mean_absolute_error_vs_iterations</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/evaluation_mean_squared_error_vs_iterations</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/evaluation_root_mean_squared_error_vs_iterations</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/train/epoch_loss</td><td>█▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>20210527-154608/train/epoch_mean_absolute_error</td><td>█▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>20210527-154608/train/epoch_mean_squared_error</td><td>█▄▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>20210527-154608/train/epoch_root_mean_squared_error</td><td>█▅▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>20210527-154608/validation/epoch_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/epoch_mean_absolute_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/epoch_mean_squared_error</td><td>█▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>20210527-154608/validation/epoch_root_mean_squared_error</td><td>█▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 13 media file(s), 1413 artifact file(s) and 3 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:modelforecastlstmoneshot:wide_in_out_oneshot_hz:ops:3382hvgb</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/2thag039\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/2thag039</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     loss  \\\n",
       "scores                                                      \n",
       "modelforecastdense:single_in_step                   0.009   \n",
       "modelforecastdense:wide_in_step                     0.011   \n",
       "modelforecastlstm:wide_in_step                      0.013   \n",
       "modelforecastbilstm:wide_in_step                    0.013   \n",
       "modelforecastlstmoneshot:wide_in_out                0.011   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk  0.010   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz     0.010   \n",
       "\n",
       "                                                    mean_absolute_error  \\\n",
       "scores                                                                    \n",
       "modelforecastdense:single_in_step                                 0.069   \n",
       "modelforecastdense:wide_in_step                                   0.081   \n",
       "modelforecastlstm:wide_in_step                                    0.090   \n",
       "modelforecastbilstm:wide_in_step                                  0.091   \n",
       "modelforecastlstmoneshot:wide_in_out                              0.078   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk                0.074   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                   0.079   \n",
       "\n",
       "                                                    mean_squared_error  \\\n",
       "scores                                                                   \n",
       "modelforecastdense:single_in_step                                0.009   \n",
       "modelforecastdense:wide_in_step                                  0.011   \n",
       "modelforecastlstm:wide_in_step                                   0.013   \n",
       "modelforecastbilstm:wide_in_step                                 0.013   \n",
       "modelforecastlstmoneshot:wide_in_out                             0.011   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk               0.010   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                  0.010   \n",
       "\n",
       "                                                    root_mean_squared_error  \n",
       "scores                                                                       \n",
       "modelforecastdense:single_in_step                                     0.094  \n",
       "modelforecastdense:wide_in_step                                       0.105  \n",
       "modelforecastlstm:wide_in_step                                        0.113  \n",
       "modelforecastbilstm:wide_in_step                                      0.114  \n",
       "modelforecastlstmoneshot:wide_in_out                                  0.104  \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk                    0.099  \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                       0.102  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>mean_absolute_error</th>\n      <th>mean_squared_error</th>\n      <th>root_mean_squared_error</th>\n    </tr>\n    <tr>\n      <th>scores</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>modelforecastdense:single_in_step</th>\n      <td>0.009</td>\n      <td>0.069</td>\n      <td>0.009</td>\n      <td>0.094</td>\n    </tr>\n    <tr>\n      <th>modelforecastdense:wide_in_step</th>\n      <td>0.011</td>\n      <td>0.081</td>\n      <td>0.011</td>\n      <td>0.105</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstm:wide_in_step</th>\n      <td>0.013</td>\n      <td>0.090</td>\n      <td>0.013</td>\n      <td>0.113</td>\n    </tr>\n    <tr>\n      <th>modelforecastbilstm:wide_in_step</th>\n      <td>0.013</td>\n      <td>0.091</td>\n      <td>0.013</td>\n      <td>0.114</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out</th>\n      <td>0.011</td>\n      <td>0.078</td>\n      <td>0.011</td>\n      <td>0.104</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk</th>\n      <td>0.010</td>\n      <td>0.074</td>\n      <td>0.010</td>\n      <td>0.099</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out_oneshot_hz</th>\n      <td>0.010</td>\n      <td>0.079</td>\n      <td>0.010</td>\n      <td>0.102</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# define configurations to be used\n",
    "single_in_step            = dict(num_timesteps=1,           out_width=1,             horizon=1)\n",
    "wide_in_step              = dict(num_timesteps=time_steps,  out_width=1,             horizon=1)\n",
    "wide_in_out               = dict(num_timesteps=time_steps,  out_width=output_width,  horizon=1)\n",
    "wide_in_out_oneshot_hz_wk = dict(num_timesteps=time_steps,  out_width=output_width,  horizon=horizon//7)\n",
    "wide_in_out_oneshot_hz    = dict(num_timesteps=time_steps,  out_width=output_width,  horizon=horizon)\n",
    "# define models to be used \n",
    "database = [\n",
    "    SearchQuery(model_cls=ModelForecastDense,       gen_name='single_in_step',               gen_params=single_in_step), \n",
    "    SearchQuery(model_cls=ModelForecastDense,       gen_name='wide_in_step',                 gen_params=wide_in_step), \n",
    "    SearchQuery(model_cls=ModelForecastLSTM,        gen_name='wide_in_step',                 gen_params=wide_in_step), \n",
    "    SearchQuery(model_cls=ModelForecastBiLSTM,      gen_name='wide_in_step',                 gen_params=wide_in_step), \n",
    "    SearchQuery(model_cls=ModelForecastLSTMOneShot, gen_name='wide_in_out',                  gen_params=wide_in_out), \n",
    "    SearchQuery(model_cls=ModelForecastLSTMOneShot, gen_name='wide_in_out_oneshot_hz_wk',    gen_params=wide_in_out_oneshot_hz_wk), \n",
    "    SearchQuery(model_cls=ModelForecastLSTMOneShot, gen_name='wide_in_out_oneshot_hz',       gen_params=wide_in_out_oneshot_hz) \n",
    "]\n",
    "# configure tracker object for each entry \n",
    "for entry in database: \n",
    "    job_dict             = asdict(tracker)\n",
    "    tracker_job          = lc.JobTrackInfo(**job_dict)\n",
    "    # reuse and configure from main tracker configured\n",
    "    tracker_job.group    = tracker.group \n",
    "    tracker_job.name     = tracker.name.format(entry.model_cls.__name__.lower(), entry.gen_name)\n",
    "    tracker_job.job_type = tracker.job_type\n",
    "    tracker_job.config.update({'generator':entry.gen_params})\n",
    "    # add a tracking parameter \n",
    "    entry.tracker_task   = tracker_job \n",
    "    entry.model_name     = \":\".join([entry.model_cls.__name__.lower(), entry.gen_name])\n",
    "# perform search with context of spawning jobs\n",
    "df_scores_eval = search(database, df_train_part_sc, partitions, features, config)\n",
    "df_scores_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Syncing run <strong style=\"color:#cdcd00\">lc-svc:lineage:ops:2teq2ir7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/a44xwz65\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/a44xwz65</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_160246-a44xwz65</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./experiments/snapshots/m5-fcst-base/exports/artifacts/models)... Done. 0.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./experiments/snapshots/m5-fcst-base/exports/figures)... Done. 0.0s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 51112<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_160246-a44xwz65/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_160246-a44xwz65/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 35 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:lineage:ops:2teq2ir7</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/a44xwz65\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/a44xwz65</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "# upload model artifacts  \n",
    "tracker_cicd = lc.JobTrackInfo(**dict(\n",
    "    name    = 'lineage',\n",
    "    project = project_config['name'], \n",
    "    entity  = project_config['entity'],\n",
    "    group   = \":\".join(['exp', 'modeling']),\n",
    "    # default main configuration used which will be added to \n",
    "    config  = config, \n",
    "    tags    = project_config['tags'] + ['artifact'],\n",
    "    notes   = \"service model experimentation model artifacts upload\"\n",
    "))\n",
    "tracker_cicd.format_job_type(cycle=lc.JobTrackingCycle.Experiment, action=lc.ActionType.Upload)\n",
    "with lc.LifeCycleSvc.execute(tracker_cicd) as cxt:\n",
    "    artifact_svc = lc.LifeCycleArtifactSvc()\n",
    "    for repo_name, uri_src, uri_dst in zip (\n",
    "        ['model', 'figures'], \n",
    "        [config['experiment_parameters']['model_dir'], config['experiment_parameters']['figures_dir']], \n",
    "        ['training', 'figures']):\n",
    "        # Artifact Add Config Files (./add_dir) to artifact\n",
    "        artifact_config = lc.ArtifactInfo( **dict( \n",
    "            name        = f'{repo_name}_repo', \n",
    "            type        = lc.ArtifactStorage.Experiment, \n",
    "            description = f\"model {repo_name} associated with training the dataset: {config['dataset']['id']}\",\n",
    "        ))\n",
    "        artifact_svc.add(cxt, artifact_config, uri_src_path = uri_src,  uri_dst_path = uri_dst, \n",
    "            alias         = ['master', 'staging'], \n",
    "            write_request = lc.ArtifactWrite.AddDir\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.30<br/>\n                Resuming run <strong style=\"color:#cdcd00\">lc-svc:lineage:ops:2teq2ir7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/akamlani/m5-forecast\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast</a><br/>\n                Run page: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/3evohnxh\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/3evohnxh</a><br/>\n                Run data is saved locally in <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_161129-3evohnxh</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: `DataFrame.to_dict` defaulting to pandas implementation.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 51815<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_161129-3evohnxh/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/arikamlani/Dropbox/dev-platform/repositories/flexassist-core/demos/tabular/m5-forecast/wandb/run-20210527_161129-3evohnxh/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>9</td></tr><tr><td>_timestamp</td><td>1622146294</td></tr><tr><td>_step</td><td>0</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">lc-svc:lineage:ops:2teq2ir7</strong>: <a href=\"https://wandb.ai/akamlani/m5-forecast/runs/3evohnxh\" target=\"_blank\">https://wandb.ai/akamlani/m5-forecast/runs/3evohnxh</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                     loss  \\\n",
       "scores                                                      \n",
       "modelforecastdense:single_in_step                   0.009   \n",
       "modelforecastdense:wide_in_step                     0.011   \n",
       "modelforecastlstm:wide_in_step                      0.013   \n",
       "modelforecastbilstm:wide_in_step                    0.013   \n",
       "modelforecastlstmoneshot:wide_in_out                0.011   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk  0.010   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz     0.010   \n",
       "\n",
       "                                                    mean_absolute_error  \\\n",
       "scores                                                                    \n",
       "modelforecastdense:single_in_step                                 0.069   \n",
       "modelforecastdense:wide_in_step                                   0.081   \n",
       "modelforecastlstm:wide_in_step                                    0.090   \n",
       "modelforecastbilstm:wide_in_step                                  0.091   \n",
       "modelforecastlstmoneshot:wide_in_out                              0.078   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk                0.074   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                   0.079   \n",
       "\n",
       "                                                    mean_squared_error  \\\n",
       "scores                                                                   \n",
       "modelforecastdense:single_in_step                                0.009   \n",
       "modelforecastdense:wide_in_step                                  0.011   \n",
       "modelforecastlstm:wide_in_step                                   0.013   \n",
       "modelforecastbilstm:wide_in_step                                 0.013   \n",
       "modelforecastlstmoneshot:wide_in_out                             0.011   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk               0.010   \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                  0.010   \n",
       "\n",
       "                                                    root_mean_squared_error  \n",
       "scores                                                                       \n",
       "modelforecastdense:single_in_step                                     0.094  \n",
       "modelforecastdense:wide_in_step                                       0.105  \n",
       "modelforecastlstm:wide_in_step                                        0.113  \n",
       "modelforecastbilstm:wide_in_step                                      0.114  \n",
       "modelforecastlstmoneshot:wide_in_out                                  0.104  \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk                    0.099  \n",
       "modelforecastlstmoneshot:wide_in_out_oneshot_hz                       0.102  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>mean_absolute_error</th>\n      <th>mean_squared_error</th>\n      <th>root_mean_squared_error</th>\n    </tr>\n    <tr>\n      <th>scores</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>modelforecastdense:single_in_step</th>\n      <td>0.009</td>\n      <td>0.069</td>\n      <td>0.009</td>\n      <td>0.094</td>\n    </tr>\n    <tr>\n      <th>modelforecastdense:wide_in_step</th>\n      <td>0.011</td>\n      <td>0.081</td>\n      <td>0.011</td>\n      <td>0.105</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstm:wide_in_step</th>\n      <td>0.013</td>\n      <td>0.090</td>\n      <td>0.013</td>\n      <td>0.113</td>\n    </tr>\n    <tr>\n      <th>modelforecastbilstm:wide_in_step</th>\n      <td>0.013</td>\n      <td>0.091</td>\n      <td>0.013</td>\n      <td>0.114</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out</th>\n      <td>0.011</td>\n      <td>0.078</td>\n      <td>0.011</td>\n      <td>0.104</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out_oneshot_hz_wk</th>\n      <td>0.010</td>\n      <td>0.074</td>\n      <td>0.010</td>\n      <td>0.099</td>\n    </tr>\n    <tr>\n      <th>modelforecastlstmoneshot:wide_in_out_oneshot_hz</th>\n      <td>0.010</td>\n      <td>0.079</td>\n      <td>0.010</td>\n      <td>0.102</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "with lc.LifeCycleSvc.execute(tracker_cicd) as cxt:\n",
    "    import pandas \n",
    "    wandb.log({\"scores\":   wandb.Table(dataframe=pandas.DataFrame.from_dict(df_scores_eval.to_dict()))}, commit=True)\n",
    "df_scores_eval"
   ]
  }
 ]
}